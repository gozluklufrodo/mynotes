1. Transformer models can be grouped into three categories:
* GPT-like (auto-regressive transformer models)

* BERT-like (auto-encoding transformer models)

* BART/T5 - like (sequence-to-sequence Transformer models)
2. Encoder Models --> BERT, ALBERT, DistilBERT
* This models use only encoder part of the transformer model. 

* This works bertter for Natural Language Understanding tasks, like Sentiment Analysis, Text Classification, Named Entity Recognition etc.
3. Decoder Models --> GPT, GPT2 etc. 
* This models use only decoder part of the transformer model.

* Works better for Natural Language Generation tasks. Predicting the next word given some words. 
4. Encoder-decoderr models --> BART, T5 etc.

5. Uses both encoder and decoder parts of the transformer model.

6. Works better for seq2seq problems, like summarization and translation.

7. 
